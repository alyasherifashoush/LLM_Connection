{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56acc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500594a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81363f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc3e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class EvaluationScores(BaseModel):\n",
    "    correctness: Literal[1, 2, 3, 4, 5]\n",
    "    correctness_reason: str\n",
    "    completeness_reference: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_reference_reason: str\n",
    "    faithfulness: Literal[1, 2, 3, 4, 5]\n",
    "    faithfulness_reason: str\n",
    "    completeness_question: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_question_reason: str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1e80c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original version of the prompt: uses the context for completeness to reference instead of only the reference\n",
    "def build_prompt_v1(response, reference, context, question):\n",
    "  return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE?\n",
    "3. **Faithfulness to Context** – Does the RESPONSE strictly reflect what is present in the CONTEXT (no made-up or contradictory info)?\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully address all aspects of the QUESTION?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "REFERENCE:\n",
    "{reference}\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b33049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second version of the prompt: \n",
    "# Added ignore...\n",
    "# Reduces points because wants answer to have everything mentioned in the context even if not relevant to the question.\n",
    "#  Faithfulness is only about checking if the answer does not contradict the context or invent info. \n",
    "#  Completeness (question) is about checking if the answer is complete and answers the question fully\n",
    "\n",
    "def build_prompt_v2(response, reference, context, question):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?  Ignore the context and question for this rating\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE? Ignore the context and question for this rating \n",
    "3. **Faithfulness to Context** – Does the RESPONSE strictly reflect what is present in the CONTEXT (no made-up or contradictory info)? Ignore the reference and question for this rating\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully address all aspects of the QUESTION?  Ignore the reference and context for this rating\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "REFERENCE (for correctness & completeness_reference):\n",
    "{reference}\n",
    "\n",
    "CONTEXT (for faithfulness):\n",
    "{context}\n",
    "\n",
    "QUESTION (for completeness_question):\n",
    "{question}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b67b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version of the prompt: refined the defintions of faithfulness and completeness to question.\n",
    "\n",
    "def build_prompt_v3(response, reference, context, question):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?  Ignore the context and question for this rating\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE? Ignore the context and question for this rating \n",
    "3. **Faithfulness to Context** – Check ONLY whether the RESPONSE is faithful to the CONTEXT.Ignore the reference and question for this rating\n",
    "   - Faithful means: no made-up facts, no contradictions with the context.\n",
    "   - Do NOT penalize for leaving out context information.\n",
    "   - Faithfulness is about truthfulness, NOT completeness.\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully answer what is asked in the QUESTION? Ignore the reference and context for this rating\n",
    "   - Use CONTEXT **only as needed** to support this.\n",
    "   - Do NOT penalize if RESPONSE omits unrelated context details.\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "REFERENCE (for correctness & completeness_reference):\n",
    "{reference}\n",
    "\n",
    "CONTEXT (for faithfulness):\n",
    "{context}\n",
    "\n",
    "QUESTION (for completeness_question):\n",
    "{question}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b224bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "import json\n",
    "\n",
    "def evaluate_response(question, response, reference, context,prompt_builder):\n",
    "    prompt = build_prompt_v3(response, reference, context, question)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator assessing hallucination in RAG-based answers. Always return your ratings in valid JSON format. Do not include anything else.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content.strip()\n",
    "        json_start_index = output.find('{')\n",
    "        output = output[json_start_index:].strip().rstrip(\"```\")\n",
    "        result = EvaluationScores(**json.loads(output))\n",
    "        return result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ Failed to parse JSON:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    except ValidationError as e:\n",
    "        print(\"❌ Schema validation failed:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1a541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_all(input_csv: str, output_csv: str, prompt_builder, prompt_id: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    results = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        res = evaluate_response(\n",
    "            question=row[\"Question\"],\n",
    "            response=row[\"Generated Answer\"],\n",
    "            reference=row[\"Ref_Answer\"],\n",
    "            context=row[\"Top-k Chunks Used\"],\n",
    "            prompt_builder=prompt_builder \n",
    "        )\n",
    "\n",
    "        if res:\n",
    "            result_row = {\n",
    "                \"question_index\": row[\"Question Number\"],\n",
    "                \"correct_chunk\": row[\"Original Chunk ID\"],\n",
    "                \"reference_answer\": row[\"Ref_Answer\"],\n",
    "                \"retrieved_chunks\": row[\"Top-k Chunks Used\"],\n",
    "                \"correctness\": res.correctness,\n",
    "                \"correctness_reason\": res.correctness_reason,\n",
    "                \"completeness_reference\": res.completeness_reference,\n",
    "                \"completeness_reference_reason\": res.completeness_reference_reason,\n",
    "                \"faithfulness\": res.faithfulness,\n",
    "                \"faithfulness_reason\": res.faithfulness_reason,\n",
    "                \"completeness_question\": res.completeness_question,\n",
    "                \"completeness_question_reason\": res.completeness_question_reason,\n",
    "                \"judge_model_used\": MODEL,\n",
    "                \"prompt_variant_id\": prompt_id\n",
    "            }\n",
    "            results.append(result_row)\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✅ Evaluation complete. Output saved to: {output_csv}\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5bbc906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete. Output saved to: evaluated_output_v2.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'model_dump_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_780343/854773294.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m         output_csv=\u001b[33m\"evaluated_output_v2.csv\"\u001b[39m,\n\u001b[32m     28\u001b[39m         prompt_builder=build_prompt_v2,\n\u001b[32m     29\u001b[39m         prompt_id=\u001b[33m\"prompt_v2\"\u001b[39m\n\u001b[32m     30\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     print(result.model_dump_json(indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'model_dump_json'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     # Example single evaluation\n",
    "#     example_question = \"What is considered an official trip?\"\n",
    "#     example_response = f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business.\"\"\"\n",
    "#     example_reference = f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business.\"\"\"\n",
    "#     example_context =  f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business. The order or approval must be given \n",
    "# in writing or electronically. Official trips also include journeys from a location serving as a \n",
    "# temporary residence to the place of work, provided that the conditions of sentences 1 \n",
    "# and 2 are otherwise met. Official trips should only be carried out if a less costly method of \n",
    "# conducting the official business is not possible or reasonable.\"\"\"\n",
    "\n",
    "#     result = evaluate_response(example_question, example_response, example_reference, example_context)\n",
    "\n",
    "\n",
    "    # Uncomment to run batch evaluation:\n",
    "    result=evaluate_all(\n",
    "        input_csv=\"AddedColManually_RAG_Output_Answers.csv\",\n",
    "        output_csv=\"evaluated_output_v2.csv\",\n",
    "        prompt_builder=build_prompt_v2,\n",
    "        prompt_id=\"prompt_v2\"\n",
    "    )\n",
    "    print(result.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
