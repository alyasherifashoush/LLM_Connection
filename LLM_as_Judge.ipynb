{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56acc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500594a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81363f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc3e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class EvaluationScores(BaseModel):\n",
    "    correctness: Literal[1, 2, 3, 4, 5]\n",
    "    correctness_reason: str\n",
    "    completeness_reference: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_reference_reason: str\n",
    "    faithfulness: Literal[1, 2, 3, 4, 5]\n",
    "    faithfulness_reason: str\n",
    "    completeness_question: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_question_reason: str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e80c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original version of the prompt: uses the context for completeness to reference instead of only the reference\n",
    "def build_prompt_v1(response, reference, context, question):\n",
    "  return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE?\n",
    "3. **Faithfulness to Context** – Does the RESPONSE strictly reflect what is present in the CONTEXT (no made-up or contradictory info)?\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully address all aspects of the QUESTION?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "REFERENCE:\n",
    "{reference}\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second version of the prompt: \n",
    "# Added ignore...\n",
    "# Reduces points because wants answer to have everything mentioned in the context even if not relevant to the question.\n",
    "#  Faithfulness is only about checking if the answer does not contradict the context or invent info. \n",
    "#  Completeness (question) is about checking if the answer is complete and answers the question fully\n",
    "\n",
    "def build_prompt_v2(response, reference, context, question):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?  Ignore the context and question for this rating\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE? Ignore the context and question for this rating \n",
    "3. **Faithfulness to Context** – Does the RESPONSE strictly reflect what is present in the CONTEXT (no made-up or contradictory info)? Ignore the reference and question for this rating\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully address all aspects of the QUESTION?  Ignore the reference and context for this rating\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "REFERENCE (for correctness & completeness_reference):\n",
    "{reference}\n",
    "\n",
    "CONTEXT (for faithfulness):\n",
    "{context}\n",
    "\n",
    "QUESTION (for completeness_question):\n",
    "{question}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b67b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version of the prompt: refined the defintions of faithfulness and completeness to question.\n",
    "\n",
    "def build_prompt_v3(response, reference, context, question):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?  Ignore the context and question for this rating\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE? Ignore the context and question for this rating \n",
    "3. **Faithfulness to Context** – Check ONLY whether the RESPONSE is faithful to the CONTEXT.Ignore the reference and question for this rating\n",
    "   - Faithful means: no made-up facts, no contradictions with the context.\n",
    "   - Do NOT penalize for leaving out context information.\n",
    "   - Faithfulness is about truthfulness, NOT completeness.\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully answer what is asked in the QUESTION? Ignore the reference and context for this rating\n",
    "   - Use CONTEXT **only as needed** to support this.\n",
    "   - Do NOT penalize if RESPONSE omits unrelated context details.\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "REFERENCE (for correctness & completeness_reference):\n",
    "{reference}\n",
    "\n",
    "CONTEXT (for faithfulness):\n",
    "{context}\n",
    "\n",
    "QUESTION (for completeness_question):\n",
    "{question}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b224bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "import json\n",
    "\n",
    "def evaluate_response(question, response, reference, context,prompt_builder):\n",
    "    prompt = prompt_builder(response, reference, context, question)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator assessing hallucination in RAG-based answers. Always return your ratings in valid JSON format. Do not include anything else.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "         # Extract the JSON response\n",
    "        output = response.choices[0].message.content.strip()\n",
    "\n",
    "        json_start_index = output.find('{')\n",
    "        output = output[json_start_index:].strip().rstrip(\"```\")\n",
    "\n",
    "        result = EvaluationScores(**json.loads(output))\n",
    "         # Takes JSON string and converts it to a dictionary\n",
    "        # Then double asterrisk operator does tuple unpacking and gives it to EvaluationScores constructor to convert the dictionary to a class object\n",
    "        return result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ Failed to parse JSON:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    except ValidationError as e:\n",
    "        print(\"❌ Schema validation failed:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_all(input_csv: str, output_csv: str, prompt_builder, prompt_id: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        # Call method to evaluate each row and pass to it the required parameters\n",
    "        res = evaluate_response(\n",
    "            question=row[\"Question\"],\n",
    "            response=row[\"Generated Answer\"],\n",
    "            reference=row[\"Reference Answer\"],\n",
    "            context=row[\"Chunks Retrieved\"],\n",
    "            prompt_builder=prompt_builder \n",
    "        )\n",
    "\n",
    "        if res:\n",
    "\n",
    "            # Create the output row with the scores\n",
    "            result_row = {\n",
    "                \"Generation Model\": row[\"Generation Model\"],\n",
    "                \"Question Index\": row[\"Question Index\"],\n",
    "                \"Question\": row[\"Question\"],\n",
    "                \"Type\": row[\"Type\"],\n",
    "                \"Source_QID\": row[\"Source_QID\"],\n",
    "                \"Original Chunk\": row[\"Original Chunk\"],\n",
    "                \"Chunks Retrieved\": row[\"Chunks Retrieved\"],\n",
    "                \"Generated Answer\": row[\"Generated Answer\"],\n",
    "                \"Reference Answer\": row[\"Reference Answer\"],\n",
    "                \"Generation Prompt Used\": row[\"Generation Prompt Used\"],\n",
    "                \"Encoding Used\": row[\"Encoding Used\"],\n",
    "                \"correctness\": res.correctness,\n",
    "                \"correctness_reason\": res.correctness_reason,\n",
    "                \"completeness_reference\": res.completeness_reference,\n",
    "                \"completeness_reference_reason\": res.completeness_reference_reason,\n",
    "                \"faithfulness\": res.faithfulness,\n",
    "                \"faithfulness_reason\": res.faithfulness_reason,\n",
    "                \"completeness_question\": res.completeness_question,\n",
    "                \"completeness_question_reason\": res.completeness_question_reason,\n",
    "                \"Judge Model\": MODEL,\n",
    "                \"Judge Prompt\": prompt_id\n",
    "            }\n",
    "            results.append(result_row)\n",
    "            \n",
    "    result_df = pd.DataFrame(results) # Create a DataFrame from the results, a DataFrame is a table-like structure in pandas\n",
    "    result_df.to_csv(output_csv, index=False) # Save the results to a CSV file\n",
    "    print(f\"\\n✅ Evaluation complete. Output saved to: {output_csv}\") # Print a success message\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bbc906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete. Output saved to: evaluated_output_prompt_v2.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     # Example single evaluation\n",
    "#     example_question = \"What is considered an official trip?\"\n",
    "#     example_response = f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business.\"\"\"\n",
    "#     example_reference = f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business.\"\"\"\n",
    "#     example_context =  f\"\"\"Official trips in the sense of this Act are journeys undertaken to conduct official  \n",
    "# business outside the usual place of work, which have been ordered or approved by the  \n",
    "# responsible superior, unless an order or approval is not applicable due to the nature of the  \n",
    "# official’s office or the nature of the official business. The order or approval must be given \n",
    "# in writing or electronically. Official trips also include journeys from a location serving as a \n",
    "# temporary residence to the place of work, provided that the conditions of sentences 1 \n",
    "# and 2 are otherwise met. Official trips should only be carried out if a less costly method of \n",
    "# conducting the official business is not possible or reasonable.\"\"\"\n",
    "\n",
    "#     result = evaluate_response(example_question, example_response, example_reference, example_context, prompt_builder=build_prompt_v2)\n",
    "\n",
    "\n",
    "    # Uncomment to run batch evaluation:\n",
    "    result=evaluate_all(\n",
    "        input_csv=\"combined_generation_outputs.csv\",\n",
    "        output_csv=\"evaluated_output_prompt_v2.csv\",\n",
    "        prompt_builder=build_prompt_v2,\n",
    "        prompt_id=\"prompt_v2\"\n",
    "    )\n",
    "    # print(result.to_json(indent=2, force_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a852b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"evaluated_output_prompt_v2.csv\")\n",
    "\n",
    "# Group by the model and calculate the mean for the four metrics\n",
    "summary = df.groupby(\"Generation Model\")[[\n",
    "    \"correctness\",\n",
    "    \"completeness_reference\",\n",
    "    \"faithfulness\",\n",
    "    \"completeness_question\"\n",
    "]].mean().reset_index()\n",
    "\n",
    "# Save the summary DataFrame to a new CSV file\n",
    "summary.to_csv(\"model_performance_summary.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
