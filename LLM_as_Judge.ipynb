{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56acc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500594a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81363f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc3e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class EvaluationScores(BaseModel):\n",
    "    correctness: Literal[1, 2, 3, 4, 5]\n",
    "    correctness_reason: str\n",
    "    completeness_reference: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_reference_reason: str\n",
    "    faithfulness: Literal[1, 2, 3, 4, 5]\n",
    "    faithfulness_reason: str\n",
    "    completeness_question: Literal[1, 2, 3, 4, 5]\n",
    "    completeness_question_reason: str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(response, reference, context, question):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator assessing hallucination in RAG-based answers. Rate the following RESPONSE using a 1–5 Likert scale on four dimensions. Each rating must be justified with a short reason.\n",
    "\n",
    "Use this scale:\n",
    "1 - Very Poor\n",
    "2 - Poor\n",
    "3 - Fair\n",
    "4 - Good\n",
    "5 - Excellent\n",
    "\n",
    "Respond strictly in this format (only return valid JSON, no extra text):\n",
    "{{\n",
    "  \"correctness\": <1-5>,\n",
    "  \"correctness_reason\": \"<short justification>\",\n",
    "  \"completeness_reference\": <1-5>,\n",
    "  \"completeness_reference_reason\": \"<short justification>\",\n",
    "  \"faithfulness\": <1-5>,\n",
    "  \"faithfulness_reason\": \"<short justification>\",\n",
    "  \"completeness_question\": <1-5>,\n",
    "  \"completeness_question_reason\": \"<short justification>\"\n",
    "}}\n",
    "\n",
    "Criteria:\n",
    "1. **Correctness** – How accurately does the RESPONSE convey the same meaning as the REFERENCE, regardless of wording?\n",
    "2. **Completeness (Reference)** – Does the RESPONSE include all key information from the REFERENCE?\n",
    "3. **Faithfulness to Context** – Does the RESPONSE strictly reflect what is present in the CONTEXT (no made-up or contradictory info)?\n",
    "4. **Completeness (Question)** – Does the RESPONSE fully address all aspects of the QUESTION?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "REFERENCE:\n",
    "{reference}\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b224bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "import json\n",
    "\n",
    "def evaluate_response(question, response, reference, context):\n",
    "    prompt = build_prompt(response, reference, context, question)\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=MODEL,\n",
    "            prompt=prompt,\n",
    "            temperature=0,\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        output = response.choices[0].text.strip()\n",
    "        # Remove non-JSON parts (e.g., \"JSON:\")\n",
    "        json_start_index = output.find('{')  # Find where the JSON starts\n",
    "        output = output[json_start_index:].strip().rstrip(\"```\")\n",
    "        result = EvaluationScores(**json.loads(output))\n",
    "        return result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ Failed to parse JSON:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    except ValidationError as e:\n",
    "        print(\"❌ Schema validation failed:\")\n",
    "        print(output)\n",
    "        print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_all(input_csv: str, output_csv: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    results = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"Evaluating question {row['Question Number']}...\")\n",
    "\n",
    "        res = evaluate_response(\n",
    "            question=row[\"Question\"],\n",
    "            response=row[\"Generated Answer\"],\n",
    "            reference=row[\"Ref_Answer\"],\n",
    "            context=row[\"Top-k Chunks Used\"]\n",
    "        )\n",
    "\n",
    "        if res:\n",
    "            result_row = {\n",
    "                \"question_index\": row[\"Question Number\"],\n",
    "                \"correct_chunk\": row[\"Original Chunk ID\"],\n",
    "                \"reference_answer\": row[\"Ref_Answer\"],\n",
    "                \"retrieved_chunks\": row[\"Top-k Chunks Used\"],\n",
    "                \"correctness\": res.correctness,\n",
    "                \"correctness_reason\": res.correctness_reason,\n",
    "                \"completeness_reference\": res.completeness_reference,\n",
    "                \"completeness_reference_reason\": res.completeness_reference_reason,\n",
    "                \"faithfulness\": res.faithfulness,\n",
    "                \"faithfulness_reason\": res.faithfulness_reason,\n",
    "                \"completeness_question\": res.completeness_question,\n",
    "                \"completeness_question_reason\": res.completeness_question_reason,\n",
    "                \"judge_model_used\": MODEL\n",
    "            }\n",
    "            results.append(result_row)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✅ Evaluation complete. Output saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5bbc906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"correctness\": 4,\n",
      "  \"correctness_reason\": \"The response accurately conveys the meaning of the reference, though it uses slightly different wording ('work purposes' vs 'official duties').\",\n",
      "  \"completeness_reference\": 4,\n",
      "  \"completeness_reference_reason\": \"The response includes the key information from the reference: travel for work and the need for approval/authorization.\",\n",
      "  \"faithfulness\": 5,\n",
      "  \"faithfulness_reason\": \"The response is entirely based on the provided context and does not introduce any new or contradictory information.\",\n",
      "  \"completeness_question\": 5,\n",
      "  \"completeness_question_reason\": \"The response directly and fully answers the question of what constitutes an official trip.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example single evaluation\n",
    "    example_question = \"What is considered an official trip?\"\n",
    "    example_response = \"An official trip is any journey taken for work purposes, approved by a supervisor.\"\n",
    "    example_reference = \"An official trip is travel undertaken for official duties, typically with prior approval.\"\n",
    "    example_context = \"Official trips are defined as journeys carried out due to job responsibilities, requiring authorization.\"\n",
    "\n",
    "    result = evaluate_response(example_question, example_response, example_reference, example_context)\n",
    "    print(result.model_dump_json(indent=2))\n",
    "\n",
    "    # # Uncomment to run batch evaluation:\n",
    "    # evaluate_all(\"AddedColManually_RAG_Output_Answers.csv\", \"evaluated_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
