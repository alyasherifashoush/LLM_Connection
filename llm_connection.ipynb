{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 3 Chunks ---\n",
      "chunk_1:\n",
      "English Travel Reimbursement Law Revised Version of the State Travel Expense Act \n",
      "Preliminary Page A. Objective The previous travel expense regulations are outdated and require \n",
      "updating and legal simplification to facilitate the conduct and administrative processing of official \n",
      "travel. In addition, with regard to mobility behavior, the requirements of climate protection shall \n",
      "be taken into account (the exemplary function of the state administration pursuant to §  of the \n",
      "Baden-Württemberg Climate Protection Act).\n",
      "==============================\n",
      "\n",
      "chunk_2:\n",
      "Revised Version of the State Travel Expense Act \n",
      "Preliminary Page B. Essential Content A revision of the State Travel Expense Act resulting in a \n",
      "modern regulatory framework. The focal points are: . A new regulation for travel costs and \n",
      "mileage allowance. . . . . Adjustment of the reduction of the per diem allowance in the case of \n",
      "complimentary meals in line with tax law provisions, thereby eliminating the need to tax parts of \n",
      "the per diem. The provisions for foreign trips are integrated into the Act and the general \n",
      "administrative regulations; the previous State Foreign Travel Expense Regulation thereby \n",
      "becomes unnecessary and may lapse. Expenses incurred during an extended stay at the \n",
      "business location, separation allowance. Statutory anchoring of a climate compensation \n",
      "payment for official flights. G. Elimination of rarely occurring special regulations.\n",
      "==============================\n",
      "\n",
      "chunk_3:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page C. Alternatives None\n",
      "==============================\n",
      "\n",
      "chunk_4:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page  D. Costs for Public Budgets \n",
      "Additional costs estimated at , euros result from the climate compensation payment for officially \n",
      "required flights.\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chunk documents, save results to a dictionary where each chunk has an id and save the chunks to a CSV file\n",
    "import fitz  # PyMuPDF \n",
    "import csv\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()] # Splits the text by the marker and iterates on each chunk and cleans the whitespace. Also, it removes empty strings after cleaning.\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)} # After the colon is the value of the dictionary, which is the chunk of text. The key is the chunk number, which is created by iterating over the chunks and adding 1 to the index.\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate questions using the LLM\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "      \n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip() \n",
    "\n",
    "        # Split response into individual questions\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()] \n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\") \n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f99b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get the top-k matching chunks for a given query\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_matching_chunks(query, k):\n",
    "    # Load the model only once (move it outside this function if calling often)\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Load CSV with both chunk_id and chunk_text\n",
    "    df = pd.read_csv(\"chunks.csv\")  # df is a pandas DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. Think of it like a table in a spreadsheet or a SQL database.\n",
    "\n",
    "    # Get lists\n",
    "    chunk_ids = df[\"chunk_id\"].tolist()\n",
    "    chunk_texts = df[\"chunk_text\"].tolist()\n",
    "\n",
    "    # Encode chunks and query\n",
    "    chunk_embeddings = model.encode(chunk_texts, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    similarities = cos(chunk_embeddings, query_embedding)\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = torch.topk(similarities, k=k).indices.tolist()\n",
    "\n",
    "\n",
    "    # Map indices back to chunk_ids\n",
    "    top_chunk_ids = [chunk_ids[i] for i in top_indices]\n",
    "\n",
    "    return top_chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189a2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method reads a CSV file containing questions and their corresponding chunk IDs.\n",
    "# It returns a list of dictionaries where each dictionary contains the chunk ID as the key and the question as the value.\n",
    "\n",
    "import csv\n",
    "\n",
    "def get_queries_from_csv(csv_file):\n",
    "    queries_chunk_map = []\n",
    "\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file) # DictReader reads the CSV file into a dictionary format, where the keys are the column names and the values are the corresponding data in each row.\n",
    "\n",
    "        # Iterate over each row in the CSV file\n",
    "        for row in reader:\n",
    "            chunk_id = row['Chunk']  #Getting what coresponds to the column 'Chunk' of the dictionary\n",
    "            query= row['Question']   #Getting what coresponds to the column 'Question' of the dictionary\n",
    "\n",
    "            # Create a dictionary with chunk_id as key and question as value and appends it to the list\n",
    "            queries_chunk_map.append({chunk_id: query})\n",
    "\n",
    "    return queries_chunk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae3f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_CSVcolumn(csv_file,column_name):\n",
    "\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract the questions into a list\n",
    "    column_list = df[column_name].tolist()\n",
    "    \n",
    "    # Return the list\n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea003549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Step 1: Convert to lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Step 2: Strip leading/trailing whitespace\n",
    "    s = s.strip()\n",
    "    \n",
    "    # Step 3: Normalize Unicode characters (e.g., accented characters)\n",
    "    s = unicodedata.normalize('NFKD', s)  # Normalize to decomposed form\n",
    "    \n",
    "    # Step 4: Remove non-printable characters (e.g., invisible characters)\n",
    "    s = re.sub(r'[^\\x20-\\x7E]', '', s)  # Keep only printable characters\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "correct_answers_count = 0\n",
    "queries_chunk_map = get_queries_from_csv(\"final_test_set.csv\")[:20]  # Limit to 20 queries\n",
    "\n",
    "# Prompt LLM\n",
    "with open(\"RAG_Answers_To.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for i, query_map in enumerate(queries_chunk_map, start=1):\n",
    "        chunk_id, query = list(query_map.items())[0]  # Extract the chunk ID and question\n",
    "\n",
    "        # Get top k chunks\n",
    "        top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "\n",
    "        # Load the corresponding chunk texts from the CSV\n",
    "        csv_file = '/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv' \n",
    "        column_name = 'chunk_text'\n",
    "        result = get_CSVcolumn(csv_file, column_name)\n",
    "        top_k_documents = [result[int(chunk_id.split(\"_\")[1]) - 1] for chunk_id in top_k_chunk_ids]\n",
    "\n",
    "        # Join the top k documents into a single string\n",
    "        top_k_string = \"\\n\".join(top_k_documents)\n",
    "\n",
    "     #  PROMPT SELECTION:  Modify this section to try different prompts\n",
    "        messages = []\n",
    "\n",
    "        # #  Prompt 1: Basic Template with Explicit Instructions  [cite: 19, 20, 21]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": \"Use the following pieces of retrieved context (replace with documents) to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"},\n",
    "        #     {\"role\": \"user\", \"content\": f\"Question: {query} \\n Context: {top_k_string} \\n Answer:\"}\n",
    "        # ]\n",
    "\n",
    "        # #  Prompt 2: Template with Context Delimiters  [cite: 22, 23]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"--------------------- {top_k_string} --------------------- \\n Given the context information and not prior knowledge, answer the query: {query} \\n Answer:\"}\n",
    "        # ]\n",
    "\n",
    "        # #  Prompt 3: Template with Emphasis on Factual Grounding  [cite: 23, 24, 25]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"DOCUMENT: {top_k_string} \\n QUESTION: {query} \\n INSTRUCTIONS: Answer the users QUESTION using the DOCUMENT text above. Keep your answer grounded in the facts of the DOCUMENT. If the DOCUMENT doesn't contain the facts to answer the QUESTION return {{NONE}}\"}\n",
    "        # ]\n",
    "\n",
    "        # #  Prompt 4: Template for Extractive Answering  [cite: 25, 26, 27]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"Extract the most relevant passage from the retrieved documents {top_k_string} that answers the query {query}. Return only the exact text from {top_k_string} without modification.\"}\n",
    "        # ]\n",
    "\n",
    "        # #  Prompt 5: Template Incorporating Chain-of-Thought (CoT)  [cite: 27, 28, 29]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"Based on the retrieved context: {top_k_string}, answer the question {query} step by step, first identifying key facts, then reasoning through the answer.\"}\n",
    "        # ]\n",
    "\n",
    "        # #  Prompt 6: More Concise Template  [cite: 29, 30]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"Context: {top_k_string} \\n Question: {query} \\n Answer the question using only the context provided.\"}\n",
    "        # ]\n",
    "\n",
    "        # # Prompt 7: Template with Specific Formatting Instructions  [cite: 30, 31, 32]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": f\"Context information is below. \\n --------------------- \\n {top_k_string} \\n --------------------- \\n Given the context information and not prior knowledge, answer the query: {query} \\n Answer:\"}\n",
    "        # ]\n",
    "\n",
    "\n",
    "        # # Prompt 8: Query-Focused Template [cite: 2, 3]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": \"I want you to answer the question based on the retrieved context below.\"},\n",
    "        #     {\"role\": \"user\", \"content\": f\"Context: {top_k_string} \\n Question: {query} \\n Answer:\"}\n",
    "        # ]\n",
    "\n",
    "        # # Prompt 9: Multi-Vector Retrieval Template [cite: 7, 8, 9]\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": \"I need you to answer a question based on the following context information. If the information needed is not available in the context, please state that.\"},\n",
    "        #     {\"role\": \"user\", \"content\": f\"Context: {top_k_string} \\n Question: {query} \\n Answer (be concise and extract relevant information from the context):\"}\n",
    "        # ]\n",
    "\n",
    "        # Prompt 10: FLARE Template [cite: 9, 10, 11, 12]\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer the question based on the context below. If you need more information that's not in the context, indicate this in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {top_k_string} \\n Question: {query} \\n If you need additional information, please specify what you need to know. \\n Answer:\"}\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        # Get answer from LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            seed=42,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        raw_response = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Check if correct chunk is among top-k\n",
    "        normalized_chunk_id = normalize_string(chunk_id)\n",
    "        normalized_top_k_chunk_ids= [normalize_string(a) for a in top_k_chunk_ids]\n",
    "        correct_chunk_found = normalized_chunk_id in normalized_top_k_chunk_ids\n",
    "\n",
    "        if correct_chunk_found:\n",
    "            correct_answers_count += 1\n",
    "        \n",
    "\n",
    "        # Write to file\n",
    "        f.write(f\"Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "        f.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "        f.write(f\"Question: {query}\\n\")\n",
    "        f.write(f\"Answer: {raw_response}\\n\\n\")\n",
    "    f.write(f\"Total Correct Answers: {correct_answers_count}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
