{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunk documents, save results to a dictionary where each chunk has an id and save the chunks to a CSV file\n",
    "import fitz  # PyMuPDF \n",
    "import csv\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()] # Splits the text by the marker and iterates on each chunk and cleans the whitespace. Also, it removes empty strings after cleaning.\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)} # After the colon is the value of the dictionary, which is the chunk of text. The key is the chunk number, which is created by iterating over the chunks and adding 1 to the index.\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate questions using the LLM and save to a text file named \"response.txt\"\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "      \n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip() \n",
    "\n",
    "        # Split response into individual questions\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()] \n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\") \n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c6bce",
   "metadata": {},
   "source": [
    "Code to iterate over models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "# def load_model(model_name):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         torch_dtype=\"auto\",\n",
    "#         cache_dir=MODEL_CACHE_DIR\n",
    "#     ).to(\"cuda:0\")\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(system_prompt: str, user_prompt: str, model, tokenizer, temperature: float = 0.7, seed: int = 42, enable_thinking=False) -> str:\n",
    "#     set_seed(seed=seed)\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt}\n",
    "#     ]\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "#     generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "#     output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "#     content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "#     return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gc\n",
    "# import torch\n",
    "\n",
    "\n",
    "# k = 10\n",
    "# prompt_id = \"v1\"\n",
    "\n",
    "# # Models to iterate over\n",
    "# model_names = [\n",
    "#     \"Qwen/Qwen3-0.6B\",\n",
    "#     \"Qwen/Qwen3-1.7B\",\n",
    "#     \"Qwen/Qwen3-4B\",\n",
    "#     \"Qwen/Qwen3-8B\",\n",
    "# ]\n",
    "\n",
    "# # Load test set\n",
    "# with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     full_data = list(reader)\n",
    "\n",
    "# queries_chunk_map = [\n",
    "#     {\n",
    "#         \"chunk_id\": row[\"Chunk\"],\n",
    "#         \"question\": row[\"Question\"],\n",
    "#         \"reference\": row[\"Answer\"]\n",
    "#     }\n",
    "#     for row in full_data\n",
    "# ]\n",
    "\n",
    "# # Output files\n",
    "# output_txt_path = \"RAG_Output_Answers_ALL_MODELS.txt\"\n",
    "# output_csv_path = \"RAG_Output_Answers_ALL_MODELS.csv\"\n",
    "\n",
    "# with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file, \\\n",
    "#      open(output_csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "\n",
    "#     fieldnames = [\n",
    "#         \"Model Used\", \"Question Index\", \"Question\", \"Original Chunk\",\n",
    "#         \"Chunks Retrieved\", \"Answer\", \"Reference Answer\", \"Prompt Used\"\n",
    "#     ]\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for model_name in model_names:\n",
    "#         print(f\"\\nðŸ”„ Loading model: {model_name}\")\n",
    "#         model, tokenizer = load_model(model_name)\n",
    "\n",
    "#         correct_answers_count = 0\n",
    "\n",
    "#         for i, item in enumerate(queries_chunk_map, start=1):\n",
    "#             chunk_id = item[\"chunk_id\"]\n",
    "#             query = item[\"question\"]\n",
    "#             reference_answer = item[\"reference\"]\n",
    "\n",
    "#             top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "#             result = get_CSVcolumn('/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv', 'chunk_text')\n",
    "#             top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids]\n",
    "#             top_k_string = \"\\n\".join(top_k_documents)\n",
    "\n",
    "#             prompt_text = f\"Context: {top_k_string} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "#             raw_response = generate(\n",
    "#                 system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "#                 user_prompt=prompt_text,\n",
    "#                 model=model,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 temperature=0.7,\n",
    "#                 seed=42\n",
    "#             ).strip()\n",
    "\n",
    "#             # Evaluate if correct chunk was found\n",
    "#             normalized_chunk_id = normalize_string(chunk_id)\n",
    "#             normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "#             if normalized_chunk_id in normalized_top_k_chunk_ids:\n",
    "#                 correct_answers_count += 1\n",
    "\n",
    "#             # Write to text file\n",
    "#             txt_file.write(f\"[{model_name}] Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "#             txt_file.write(f\"Question: {query}\\n\")\n",
    "#             txt_file.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "#             txt_file.write(f\"Answer: {raw_response}\\n\\n\")\n",
    "\n",
    "#             # Write to CSV\n",
    "#             writer.writerow({\n",
    "#                 \"Model Used\": model_name,\n",
    "#                 \"Question Index\": i,\n",
    "#                 \"Question\": query,\n",
    "#                 \"Original Chunk\": chunk_id,\n",
    "#                 \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "#                 \"Answer\": raw_response,\n",
    "#                 \"Reference Answer\": reference_answer,\n",
    "#                 \"Prompt Used\": prompt_id\n",
    "#             })\n",
    "\n",
    "#         print(f\"âœ… Finished {model_name}: {correct_answers_count}/{len(queries_chunk_map)} correct chunks\")\n",
    "\n",
    "#         # ðŸ§¹ Free memory\n",
    "#         del model\n",
    "#         del tokenizer\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaee8a",
   "metadata": {},
   "source": [
    "Code to Generate Model By Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f99b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_methods import get_CSVcolumn, normalize_string, get_top_matching_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_CACHE_DIR = '/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/llm/students'\n",
    "os.environ['TRANSFORMERS_CACHE'] = MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "# The model name points to a repository on huggingface, where it will load the model and configuration from, e.g.: https://huggingface.co/Qwen/Qwen3-0.6B\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "#model_name = \"Qwen/Qwen3-1.7B\"\n",
    "#model_name = \"Qwen/Qwen3-4B\"\n",
    "# model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "\n",
    "# load the tokenizer and the model (you can leave this block unchanged)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    cache_dir=MODEL_CACHE_DIR\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is for:\n",
    "# 1. converting the queries into the right template format\n",
    "# 2. tokenizing the prompt text\n",
    "# 3. converting the tokens to token ids\n",
    "# 4. running the token ids through the model, generating output token ids\n",
    "# 5. converting the output token ids back to text tokens\n",
    "def generate(system_prompt: str, user_prompt: str, temperature: float = 0.7, seed: int = 42, enable_thinking=False) -> str:\n",
    "    # set a random seed for reproducability (otherwise, calling generation twice can result in different texts)\n",
    "    set_seed(seed=seed)\n",
    "\n",
    "    # convert the prompts into the correct template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template( \n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    # convert prompt texts to tokens, move the token ids to the GPU\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    # generate the output token ids\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    # convert the output token ids to text tokens\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "k = 10\n",
    "correct_answers_count = 0\n",
    "\n",
    "prompt_id = \"v1_Basic_RAG_Prompt\"\n",
    "encoding_used=\"mpnet\"\n",
    "model_shortname = model_name.split(\"/\")[-1]  # Remove slash inside the name cause gives error\n",
    "\n",
    "\n",
    "# === Load test set ===\n",
    "with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    full_data = list(reader)\n",
    "\n",
    "\n",
    "queries_chunk_map = [ \n",
    "    {\n",
    "        \"chunk_id\": row[\"Chunk\"],\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"reference\": row[\"Answer\"],\n",
    "        \"type\": row[\"Type\"],  \n",
    "        \"source_QID\": row[\"Source_QID\"]\n",
    "    }\n",
    "    \n",
    "    for row in full_data\n",
    "]\n",
    "\n",
    "# === Output files ===\n",
    "txt_path = f\"RAG_Output_Answers_{model_shortname}.txt\"\n",
    "csv_path = f\"RAG_Output_Answers_{model_shortname}.csv\"\n",
    "\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file, open(csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = [\n",
    "       \"Generation Model\", \"Question Index\", \"Question\", \"Type\", \"Source_QID\", \n",
    "        \"Chunks Retrieved\", \"Generated Answer\", \"Reference Answer\", \"Generation Prompt Used\", \"Encoding Used\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, item in enumerate(queries_chunk_map, start=1):\n",
    "        chunk_id = item[\"chunk_id\"]\n",
    "        query = item[\"question\"]\n",
    "        reference_answer = item[\"reference\"]\n",
    "\n",
    "        # Get top-k chunks\n",
    "        top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "\n",
    "        # Load chunk texts\n",
    "        csv_file_path = '/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv'\n",
    "        result = get_CSVcolumn(csv_file_path, 'chunk_text')\n",
    "        top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids]\n",
    "\n",
    "        # Build prompt\n",
    "        top_k_string = \"\\n\".join(top_k_documents)\n",
    "        prompt_text = f\"Context: {top_k_string} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "        # Generate response\n",
    "        raw_response = generate(\n",
    "            system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "            user_prompt=prompt_text,\n",
    "            temperature=0.7,\n",
    "            seed=42\n",
    "        ).strip()\n",
    "\n",
    "        # Correct chunk detection\n",
    "        normalized_chunk_id = normalize_string(chunk_id)\n",
    "        normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "        correct_chunk_found = normalized_chunk_id in normalized_top_k_chunk_ids\n",
    "        if correct_chunk_found:\n",
    "            correct_answers_count += 1\n",
    "\n",
    "        # Write to TXT\n",
    "        txt_file.write(f\"Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "        txt_file.write(f\"Question: {query}\\n\")\n",
    "        txt_file.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "        txt_file.write(f\"Answer: {raw_response}\\n\\n\")\n",
    "\n",
    "        # Write to CSV\n",
    "        writer.writerow({\n",
    "            \"Generation Model\": model_name,\n",
    "            \"Question Index\": i,\n",
    "            \"Question\": query,\n",
    "            \"Type\": item[\"type\"],\n",
    "            \"Source_QID\": item[\"source_QID\"],\n",
    "            \"Original Chunk\": chunk_id,\n",
    "            \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "            \"Generated Answer\": raw_response,\n",
    "            \"Reference Answer\": reference_answer,\n",
    "            \"Generation Prompt Used\": prompt_id,\n",
    "            \"Encoding Used\": encoding_used    \n",
    "        })\n",
    "\n",
    "    txt_file.write(f\"Total Correct Answers: {correct_answers_count}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
