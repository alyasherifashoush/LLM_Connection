{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 3 Chunks ---\n",
      "chunk_1:\n",
      "English Travel Reimbursement Law Revised Version of the State Travel Expense Act \n",
      "Preliminary Page A. Objective The previous travel expense regulations are outdated and require \n",
      "updating and legal simplification to facilitate the conduct and administrative processing of official \n",
      "travel. In addition, with regard to mobility behavior, the requirements of climate protection shall \n",
      "be taken into account (the exemplary function of the state administration pursuant to §  of the \n",
      "Baden-Württemberg Climate Protection Act).\n",
      "==============================\n",
      "\n",
      "chunk_2:\n",
      "Revised Version of the State Travel Expense Act \n",
      "Preliminary Page B. Essential Content A revision of the State Travel Expense Act resulting in a \n",
      "modern regulatory framework. The focal points are: . A new regulation for travel costs and \n",
      "mileage allowance. . . . . Adjustment of the reduction of the per diem allowance in the case of \n",
      "complimentary meals in line with tax law provisions, thereby eliminating the need to tax parts of \n",
      "the per diem. The provisions for foreign trips are integrated into the Act and the general \n",
      "administrative regulations; the previous State Foreign Travel Expense Regulation thereby \n",
      "becomes unnecessary and may lapse. Expenses incurred during an extended stay at the \n",
      "business location, separation allowance. Statutory anchoring of a climate compensation \n",
      "payment for official flights. G. Elimination of rarely occurring special regulations.\n",
      "==============================\n",
      "\n",
      "chunk_3:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page C. Alternatives None\n",
      "==============================\n",
      "\n",
      "chunk_4:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page  D. Costs for Public Budgets \n",
      "Additional costs estimated at , euros result from the climate compensation payment for officially \n",
      "required flights.\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chunk documents, save results to a dictionary where each chunk has an id and save the chunks to a CSV file\n",
    "import fitz  # PyMuPDF \n",
    "import csv\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()] # Splits the text by the marker and iterates on each chunk and cleans the whitespace. Also, it removes empty strings after cleaning.\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)} # After the colon is the value of the dictionary, which is the chunk of text. The key is the chunk number, which is created by iterating over the chunks and adding 1 to the index.\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate questions using the LLM\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "      \n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip() \n",
    "\n",
    "        # Split response into individual questions\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()] \n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\") \n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f99b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get the top-k matching chunks for a given query\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_matching_chunks(query, k):\n",
    "    # Load the model only once (move it outside this function if calling often)\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Load CSV with both chunk_id and chunk_text\n",
    "    df = pd.read_csv(\"chunks.csv\")  # assumes columns: chunk_id, chunk_text\n",
    "\n",
    "    # Get lists\n",
    "    chunk_ids = df[\"chunk_id\"].tolist()\n",
    "    chunk_texts = df[\"chunk_text\"].tolist()\n",
    "\n",
    "    # Encode chunks and query\n",
    "    chunk_embeddings = model.encode(chunk_texts, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    similarities = cos(chunk_embeddings, query_embedding)\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = torch.topk(similarities, k=k).indices.tolist()\n",
    "\n",
    "\n",
    "    # Map indices back to chunk_ids\n",
    "    top_chunk_ids = [chunk_ids[i] for i in top_indices]\n",
    "\n",
    "    return top_chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189a2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method reads a CSV file containing questions and their corresponding chunk IDs.\n",
    "# It returns a list of dictionaries where each dictionary contains the chunk ID as the key and the question as the value.\n",
    "\n",
    "import csv\n",
    "\n",
    "def get_queries_from_csv(csv_file):\n",
    "    queries_chunk_map = []\n",
    "\n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        # Iterate over each row in the CSV file\n",
    "        for row in reader:\n",
    "            chunk_id = row['Chunk']  # Assuming 'chunk_id' column is present\n",
    "            query= row['Question']  # Assuming 'question' column is present\n",
    "\n",
    "            # Create a dictionary with chunk_id as key and question as value\n",
    "            queries_chunk_map.append({chunk_id: query})\n",
    "\n",
    "    return queries_chunk_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fae3f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_CSVcolumn(csv_file,column_name):\n",
    "\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract the questions into a list\n",
    "    column_list = df[column_name].tolist()\n",
    "    \n",
    "    # Return the list\n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "queries_chunk_map = get_queries_from_csv(\"final_test_set.csv\")[:20]  # Limit to 20 queries\n",
    "\n",
    "# Prompt LLM\n",
    "with open(\"RAG_Answers_To.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for i, query_map in enumerate(queries_chunk_map, start=1):\n",
    "        chunk_id, query = list(query_map.items())[0]  # Extract the chunk ID and question\n",
    "\n",
    "        # Get top k chunks\n",
    "        top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "\n",
    "        # Load the corresponding chunk texts from the CSV\n",
    "        csv_file = '/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv' \n",
    "        column_name = 'chunk_text'\n",
    "        result = get_CSVcolumn(csv_file, column_name)\n",
    "        top_k_documents = [result[int(chunk_id.split(\"_\")[1]) - 1] for chunk_id in top_k_chunk_ids]\n",
    "\n",
    "        # Join the top k documents into a single string\n",
    "        top_k_string = \"\\n\".join(top_k_documents)\n",
    "\n",
    "        # Prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Given the following facts: {top_k_string}, base your answer on the information provided in the documents. Do not include any additional information or context. Answer the question as precisely as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What is the best answer to the question: {query}?\"}\n",
    "        ]\n",
    "\n",
    "        # Get answer from LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            seed=42,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        raw_response = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Write to file\n",
    "        f.write(f\"Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "        f.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "        f.write(f\"Question: {query}\\n\")\n",
    "        f.write(f\"Answer: {raw_response}\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
