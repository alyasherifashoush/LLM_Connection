{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 3 Chunks ---\n",
      "chunk_1:\n",
      "English Travel Reimbursement Law Revised Version of the State Travel Expense Act \n",
      "Preliminary Page A. Objective The previous travel expense regulations are outdated and require \n",
      "updating and legal simplification to facilitate the conduct and administrative processing of official \n",
      "travel. In addition, with regard to mobility behavior, the requirements of climate protection shall \n",
      "be taken into account (the exemplary function of the state administration pursuant to §  of the \n",
      "Baden-Württemberg Climate Protection Act).\n",
      "==============================\n",
      "\n",
      "chunk_2:\n",
      "Revised Version of the State Travel Expense Act \n",
      "Preliminary Page B. Essential Content A revision of the State Travel Expense Act resulting in a \n",
      "modern regulatory framework. The focal points are: . A new regulation for travel costs and \n",
      "mileage allowance. . . . . Adjustment of the reduction of the per diem allowance in the case of \n",
      "complimentary meals in line with tax law provisions, thereby eliminating the need to tax parts of \n",
      "the per diem. The provisions for foreign trips are integrated into the Act and the general \n",
      "administrative regulations; the previous State Foreign Travel Expense Regulation thereby \n",
      "becomes unnecessary and may lapse. Expenses incurred during an extended stay at the \n",
      "business location, separation allowance. Statutory anchoring of a climate compensation \n",
      "payment for official flights. G. Elimination of rarely occurring special regulations.\n",
      "==============================\n",
      "\n",
      "chunk_3:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page C. Alternatives None\n",
      "==============================\n",
      "\n",
      "chunk_4:\n",
      "Revised \n",
      "Version of the State Travel Expense Act Preliminary Page  D. Costs for Public Budgets \n",
      "Additional costs estimated at , euros result from the climate compensation payment for officially \n",
      "required flights.\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chunk documents and save results to a dictionary where each chunk has an id\n",
    "import fitz  # PyMuPDF\n",
    "import csv\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()]\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prompt LLM\n",
    "# prompt_templates = [\n",
    "#     \"Imagine you are planning a business trip. What five questions would you ask after reading this policy?\\n\\n{doc}\",\n",
    "#     # \"Generate five clarifying questions someone might have about this policy:\\n\\n{doc}\"\n",
    "# ]\n",
    "\n",
    "# with open(\"response.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for chunk_id, doc in chunk_dict.items():\n",
    "#         for i, template in enumerate(prompt_templates):\n",
    "#             user_prompt = template.format(doc=doc)\n",
    "\n",
    "#             messages = [\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates useful and realistic questions about travel reimbursement policies.\"},\n",
    "#                 {\"role\": \"user\", \"content\": user_prompt}\n",
    "#             ]\n",
    "\n",
    "#             response = client.chat.completions.create(\n",
    "#                 model=MODEL,\n",
    "#                 messages=messages,\n",
    "#                 temperature=0.7,\n",
    "#                 seed=42\n",
    "#             )\n",
    "\n",
    "#             output = response.choices[0].message.content.strip()\n",
    "#             f.write(f\"Chunk: {chunk_id} | Template {i+1}\\n{output}\\n\\n\")\n",
    "\n",
    "# Prompt LLM\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "        # # print response\n",
    "        # print(response.choices[0].message.content)\n",
    "        # # TODO: split the response so each question is on its own line and add the id to each line\n",
    "        # f.write(f'{response.choices[0].message.content} || {id}\\n')\n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Split response into individual questions\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()]\n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\")\n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
