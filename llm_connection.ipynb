{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunk documents, save results to a dictionary where each chunk has an id and save the chunks to a CSV file\n",
    "\n",
    "import fitz  # Library for working with PDF files.\n",
    "import csv # Library for working with CSV files.\n",
    "\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()] # Splits the text by the marker and iterates on each chunk and cleans the whitespace. Also, it removes empty strings after cleaning.\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)} # After the colon is the value of the dictionary, which is the chunk of text. The key is the chunk number, which is created by iterating over the chunks and adding 1 to the index.\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate questions using the LLM and save to a text file named \"response.txt\"\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "      \n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip() \n",
    "\n",
    "        # Clean up the LLM's response by splitting into lines, removing leading/trailing noise, and filtering out empty lines\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()] \n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\") \n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c6bce",
   "metadata": {},
   "source": [
    "Code to iterate over models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "# def load_model(model_name):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         torch_dtype=\"auto\",\n",
    "#         cache_dir=MODEL_CACHE_DIR\n",
    "#     ).to(\"cuda:0\")\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(system_prompt: str, user_prompt: str, model, tokenizer, temperature: float = 0.7, seed: int = 42, enable_thinking=False) -> str:\n",
    "#     set_seed(seed=seed)\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt}\n",
    "#     ]\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "#     generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "#     output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "#     content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "#     return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gc\n",
    "# import torch\n",
    "\n",
    "\n",
    "# k = 10\n",
    "# prompt_id = \"v1\"\n",
    "\n",
    "# # Models to iterate over\n",
    "# model_names = [\n",
    "#     \"Qwen/Qwen3-0.6B\",\n",
    "#     \"Qwen/Qwen3-1.7B\",\n",
    "#     \"Qwen/Qwen3-4B\",\n",
    "#     \"Qwen/Qwen3-8B\",\n",
    "# ]\n",
    "\n",
    "# # Load test set\n",
    "# with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     full_data = list(reader)\n",
    "\n",
    "# queries_chunk_map = [\n",
    "#     {\n",
    "#         \"chunk_id\": row[\"Chunk\"],\n",
    "#         \"question\": row[\"Question\"],\n",
    "#         \"reference\": row[\"Answer\"]\n",
    "#     }\n",
    "#     for row in full_data\n",
    "# ]\n",
    "\n",
    "# # Output files\n",
    "# output_txt_path = \"RAG_Output_Answers_ALL_MODELS.txt\"\n",
    "# output_csv_path = \"RAG_Output_Answers_ALL_MODELS.csv\"\n",
    "\n",
    "# with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file, \\\n",
    "#      open(output_csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "\n",
    "#     fieldnames = [\n",
    "#         \"Model Used\", \"Question Index\", \"Question\", \"Original Chunk\",\n",
    "#         \"Chunks Retrieved\", \"Answer\", \"Reference Answer\", \"Prompt Used\"\n",
    "#     ]\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for model_name in model_names:\n",
    "#         print(f\"\\n🔄 Loading model: {model_name}\")\n",
    "#         model, tokenizer = load_model(model_name)\n",
    "\n",
    "#         correct_answers_count = 0\n",
    "\n",
    "#         for i, item in enumerate(queries_chunk_map, start=1):\n",
    "#             chunk_id = item[\"chunk_id\"]\n",
    "#             query = item[\"question\"]\n",
    "#             reference_answer = item[\"reference\"]\n",
    "\n",
    "#             top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "#             result = get_CSVcolumn('/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv', 'chunk_text')\n",
    "#             top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids]\n",
    "#             top_k_string = \"\\n\".join(top_k_documents)\n",
    "\n",
    "#             prompt_text = f\"Context: {top_k_string} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "#             raw_response = generate(\n",
    "#                 system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "#                 user_prompt=prompt_text,\n",
    "#                 model=model,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 temperature=0.7,\n",
    "#                 seed=42\n",
    "#             ).strip()\n",
    "\n",
    "#             # Evaluate if correct chunk was found\n",
    "#             normalized_chunk_id = normalize_string(chunk_id)\n",
    "#             normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "#             if normalized_chunk_id in normalized_top_k_chunk_ids:\n",
    "#                 correct_answers_count += 1\n",
    "\n",
    "#             # Write to text file\n",
    "#             txt_file.write(f\"[{model_name}] Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "#             txt_file.write(f\"Question: {query}\\n\")\n",
    "#             txt_file.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "#             txt_file.write(f\"Answer: {raw_response}\\n\\n\")\n",
    "\n",
    "#             # Write to CSV\n",
    "#             writer.writerow({\n",
    "#                 \"Model Used\": model_name,\n",
    "#                 \"Question Index\": i,\n",
    "#                 \"Question\": query,\n",
    "#                 \"Original Chunk\": chunk_id,\n",
    "#                 \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "#                 \"Answer\": raw_response,\n",
    "#                 \"Reference Answer\": reference_answer,\n",
    "#                 \"Prompt Used\": prompt_id\n",
    "#             })\n",
    "\n",
    "#         print(f\"✅ Finished {model_name}: {correct_answers_count}/{len(queries_chunk_map)} correct chunks\")\n",
    "\n",
    "#         # 🧹 Free memory\n",
    "#         del model\n",
    "#         del tokenizer\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaee8a",
   "metadata": {},
   "source": [
    "Code to Generate Model By Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f99b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from common_methods import get_CSVcolumn, normalize_string, get_top_matching_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0628f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Imports the os module, which provides a way to interact with the operating system.\n",
    "\n",
    "MODEL_CACHE_DIR = '/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/llm/students' # Tells the Hugging Face Transformers library to store downloaded model components in that specific directory.\n",
    "os.environ['TRANSFORMERS_CACHE'] = MODEL_CACHE_DIR # Sets an environment variable that tells the transformers library to use the specified cache directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d7e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 36.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Imports necessary classes from the transformers library which is a library that provides pre-trained models and tokenizers for natural language processing tasks.\n",
    "# The following classes are imported:\n",
    "# 1. AutoModelForCausalLM: A generic class that can automatically load pre-trained language models for text generation (causal language modeling).\n",
    "# 2. AutoTokenizer: A generic class that can automatically load the appropriate tokenizer for a given pre-trained model. Tokenizers are used to convert text into numerical tokens that the model can understand and vice versa.\n",
    "# 3. set_seed: A function to set the random seed for reproducibility of the generation process.\n",
    "# 4. pipeline: A high-level API for using pre-trained models for various tasks, such as text generation, translation, etc.\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "\n",
    "# The model name points to a repository on huggingface, where it will load the model and configuration from, e.g.: https://huggingface.co/Qwen/Qwen3-0.6B\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# model_name = \"Qwen/Qwen3-1.7B\"\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "# model_name = \"Qwen/Qwen3-8B\"\n",
    "# model_name = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# model_name = \"microsoft/Phi-3-small-128k-instruct\"\n",
    "# model_name = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "\n",
    "\n",
    "# Load the tokenizer and the model (you can leave this block unchanged)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR) #trust_remote_code=True) # Loads the tokenizer for the specified model from the cache directory.\n",
    "model = AutoModelForCausalLM.from_pretrained(  # This is the key function from the transformers library that handles the loading process.\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\", # This argument specifies the data type of the model parameters. \"auto\" will automatically choose the best data type based on the hardware.\n",
    "    cache_dir=MODEL_CACHE_DIR, #This optional argument specifies a local directory where the downloaded model files (configuration, weights, etc.) will be stored.\n",
    "    # trust_remote_code=True, # This argument allows the model to use remote code for custom layers or configurations, which is necessary for some models that require specific implementations.\n",
    "    \n",
    ").to(\"cuda:0\") # This is a PyTorch method that moves the entire loaded model (all its parameters and buffers) onto the first CUDA-enabled GPU available on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67da31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is for:\n",
    "# 1. converting the queries into the right template format\n",
    "# 2. tokenizing the prompt text\n",
    "# 3. converting the tokens to token ids\n",
    "# 4. running the token ids through the model, generating output token ids\n",
    "# 5. converting the output token ids back to text tokens\n",
    "\n",
    "\n",
    "enable_thinking=False\n",
    "\n",
    "def generate(system_prompt: str, user_prompt: str, temperature: float = 0.7, seed: int = 42, enable_thinking=enable_thinking) -> str:\n",
    "    # set a random seed for reproducability (otherwise, calling generation twice can result in different texts)\n",
    "    set_seed(seed=seed)\n",
    "\n",
    "    # Convert the prompts into the correct template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template( # Applies the specific chat template defined for the Qwen model to format the messages into a single text string that the model expects as input. \n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # Adds a special token that signals the start of the generation.\n",
    "        enable_thinking=enable_thinking # A parameter specific to some Qwen models to control a \"thinking\" step during generation.\n",
    "    )\n",
    "    # Convert prompt texts to tokens, move the token ids to the GPU\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Passes the input tokens and set max new tokens number to finally generate the output token ids\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # Convert the output token ids to text tokens\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: State Travel Expense Act \n",
      "(LRKG) §  Official Trips and Errands () Official trips in the sense of this Act are journeys \n",
      "undertaken to conduct official business outside the usual place of work, which have been \n",
      "ordered or approved by the responsible superior, unless an order or approval is not applicable \n",
      "due to the nature of the official’s office or the nature of the official business. The order or \n",
      "approval must be given in writing or electronically. Official trips also include journeys from a \n",
      "location serving as a temporary residence to the place of work, provided that the conditions of \n",
      "sentences 1 and 2 are otherwise met. Official trips should only be carried out if a less costly \n",
      "method of conducting the official business is not possible or reasonable.\n",
      "State Travel Expense \n",
      "Act (LRKG) §  Official Trips and Errands () Official errands are journeys undertaken to conduct \n",
      "official business at the place of work or residence outside the official premises, which have been \n",
      "ordered or approved by the responsible superior, unless an order or approval is not applicable \n",
      "due to the nature of the official’s office or the nature of the official business. The place of \n",
      "residence is deemed equivalent to a temporary residence.\n",
      "State Travel Expense Act (LRKG) §  Official Trips Abroad  () \n",
      "Official trips abroad are trips between the domestic territory and foreign countries as well as \n",
      "within foreign countries. In doing so, at least one business location must be located abroad.\n",
      "State Travel Expense Act (LRKG) \n",
      "§  Official Trips and Errands ) For official trips by a judge to carry out judicial duties or to \n",
      "participate in a meeting of the presidium or another comparable constitutional body of the court \n",
      "to which they belong, no order or approval is required. The same applies to official trips by the \n",
      "state commissioner for data protection to perform his or her duties under the State Data \n",
      "Protection Act and to official trips by the government commissioner for the concerns of people \n",
      "with disabilities to perform his or her duties under the State Equal Opportunities for People with \n",
      "Disabilities Act.\n",
      "Explanatory Memorandum  B. Detailed Explanations Regarding §  The regulation essentially \n",
      "corresponds to the previous §  LRKG. Sentence  confirms in the Act that before ordering or \n",
      "approving an official trip, it must be examined whether a less costly means of conducting the \n",
      "official business is indicated and possible, in order to manage public funds economically.\n",
      "State Travel Expense Act (LRKG) §  Official Trips Abroad () The provisions of §§  to  apply \n",
      "mutatis mutandis to official trips abroad.\n",
      "State Travel \n",
      "Expense Act (LRKG) §  Official Trips Abroad () In the case of air travel, a country is deemed \n",
      "reached at the time the airplane lands there. Stopovers are disregarded unless they necessitate \n",
      "an overnight stay. In the case of sea travel, sentence  applies accordingly.\n",
      "State Travel Expense Act (LRKG) \n",
      "§  Entitlement to Travel Expense Reimbursement () The starting and ending points of an official \n",
      "trip are generally to be determined by the traveler in accordance with the principle of cost-\n",
      "effectiveness. Deviating from this, the official superior may designate the official premises as the \n",
      "starting and/or ending point of the trip if the travel route runs immediately past the official \n",
      "premises. In the case of an official trip that is commenced and/or concluded at the traveler’s \n",
      "residence, the reimbursement of travel costs (§ ) or the mileage allowance (§ ) shall be based \n",
      "on the distance from or to the residence, unless the official premises have been designated as \n",
      "the starting and/or ending point. In the presence of several residences or accommodations, the \n",
      "one nearest to the official premises shall be decisive.\n",
      "State \n",
      "Travel Expense Act (LRKG) §  Determination of the Travel Expense Reimbursement in Special \n",
      "Cases () If, for official reasons, the early termination of a vacation or another private trip is \n",
      "ordered, the return journey from the vacation or stay location to the official premises shall be \n",
      "treated as an official trip for which travel expense reimbursement is granted.\n",
      "Explanatory Memorandum A. General Part – Essential \n",
      "Content: A revision of the State Travel Expense Act resulting in a modern regulatory framework. \n",
      "The focal points are: . . . . . . New regulation for travel costs and mileage allowance. Adjustment \n",
      "of the reduction of the per diem in the case of complimentary meals to align with tax regulations: \n",
      "this regulation eliminates the taxation of parts of the per diem. The provisions for foreign travel \n",
      "are integrated into the Act and the general administrative regulations; the previous State Foreign \n",
      "Travel Expense Regulation becomes redundant and may lapse. Expenses during an extended \n",
      "stay at the business location, separation allowance: For longer official trips and secondments, \n",
      "instead of the previous lump-sum compensation, the actual necessary accommodation costs \n",
      "and, for the first three months, a lump sum to compensate for additional meal expenses shall be \n",
      "reimbursed. Elimination of rarely occurring special regulations. Statutory anchoring of a climate \n",
      "compensation payment for official flights.\n",
      "Context: State Travel Expense Act \n",
      "(LRKG) §  Official Trips and Errands () Official trips in the sense of this Act are journeys \n",
      "undertaken to conduct official business outside the usual place of work, which have been \n",
      "ordered or approved by the responsible superior, unless an order or approval is not applicable \n",
      "due to the nature of the official’s office or the nature of the official business. The order or \n",
      "approval must be given in writing or electronically. Official trips also include journeys from a \n",
      "location serving as a temporary residence to the place of work, provided that the conditions of \n",
      "sentences 1 and 2 are otherwise met. Official trips should only be carried out if a less costly \n",
      "method of conducting the official business is not possible or reasonable.\n",
      "State Travel Expense Act (LRKG) §  Official Trips Abroad  () \n",
      "Official trips abroad are trips between the domestic territory and foreign countries as well as \n",
      "within foreign countries. In doing so, at least one business location must be located abroad.\n",
      "State Travel Expense \n",
      "Act (LRKG) §  Official Trips and Errands () Official errands are journeys undertaken to conduct \n",
      "official business at the place of work or residence outside the official premises, which have been \n",
      "ordered or approved by the responsible superior, unless an order or approval is not applicable \n",
      "due to the nature of the official’s office or the nature of the official business. The place of \n",
      "residence is deemed equivalent to a temporary residence.\n",
      "State Travel Expense Act (LRKG) \n",
      "§  Official Trips and Errands ) For official trips by a judge to carry out judicial duties or to \n",
      "participate in a meeting of the presidium or another comparable constitutional body of the court \n",
      "to which they belong, no order or approval is required. The same applies to official trips by the \n",
      "state commissioner for data protection to perform his or her duties under the State Data \n",
      "Protection Act and to official trips by the government commissioner for the concerns of people \n",
      "with disabilities to perform his or her duties under the State Equal Opportunities for People with \n",
      "Disabilities Act.\n",
      "State Travel \n",
      "Expense Act (LRKG) §  Official Trips Abroad () In the case of air travel, a country is deemed \n",
      "reached at the time the airplane lands there. Stopovers are disregarded unless they necessitate \n",
      "an overnight stay. In the case of sea travel, sentence  applies accordingly.\n",
      "Explanatory Memorandum  B. Detailed Explanations Regarding §  The regulation essentially \n",
      "corresponds to the previous §  LRKG. Sentence  confirms in the Act that before ordering or \n",
      "approving an official trip, it must be examined whether a less costly means of conducting the \n",
      "official business is indicated and possible, in order to manage public funds economically.\n",
      "State Travel Expense Act (LRKG) §  Official Trips Abroad () The provisions of §§  to  apply \n",
      "mutatis mutandis to official trips abroad.\n",
      "State Travel Expense Act (LRKG) \n",
      "§  Entitlement to Travel Expense Reimbursement () The starting and ending points of an official \n",
      "trip are generally to be determined by the traveler in accordance with the principle of cost-\n",
      "effectiveness. Deviating from this, the official superior may designate the official premises as the \n",
      "starting and/or ending point of the trip if the travel route runs immediately past the official \n",
      "premises. In the case of an official trip that is commenced and/or concluded at the traveler’s \n",
      "residence, the reimbursement of travel costs (§ ) or the mileage allowance (§ ) shall be based \n",
      "on the distance from or to the residence, unless the official premises have been designated as \n",
      "the starting and/or ending point. In the presence of several residences or accommodations, the \n",
      "one nearest to the official premises shall be decisive.\n",
      "State \n",
      "Travel Expense Act (LRKG) §  Determination of the Travel Expense Reimbursement in Special \n",
      "Cases () If, for official reasons, the early termination of a vacation or another private trip is \n",
      "ordered, the return journey from the vacation or stay location to the official premises shall be \n",
      "treated as an official trip for which travel expense reimbursement is granted.\n",
      "Explanatory Memorandum A. General Part – Essential \n",
      "Content: A revision of the State Travel Expense Act resulting in a modern regulatory framework. \n",
      "The focal points are: . . . . . . New regulation for travel costs and mileage allowance. Adjustment \n",
      "of the reduction of the per diem in the case of complimentary meals to align with tax regulations: \n",
      "this regulation eliminates the taxation of parts of the per diem. The provisions for foreign travel \n",
      "are integrated into the Act and the general administrative regulations; the previous State Foreign \n",
      "Travel Expense Regulation becomes redundant and may lapse. Expenses during an extended \n",
      "stay at the business location, separation allowance: For longer official trips and secondments, \n",
      "instead of the previous lump-sum compensation, the actual necessary accommodation costs \n",
      "and, for the first three months, a lump sum to compensate for additional meal expenses shall be \n",
      "reimbursed. Elimination of rarely occurring special regulations. Statutory anchoring of a climate \n",
      "compensation payment for official flights.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    103\u001b[39m prompt_text = build_prompt(context, query)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# print(f\"Prompt text: {prompt_text}\")  # Debugging line to see the prompt text\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m raw_response = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlease answer the user question in a faithful way.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m    112\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Correct chunk detection\u001b[39;00m\n\u001b[32m    115\u001b[39m normalized_chunk_id = normalize_string(chunk_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(system_prompt, user_prompt, temperature, seed, enable_thinking)\u001b[39m\n\u001b[32m     27\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Passes the input tokens and set max new tokens number to finally generate the output token ids\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32768\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m output_ids = generated_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(model_inputs.input_ids[\u001b[32m0\u001b[39m]):].tolist() \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Convert the output token ids to text tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:289\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:222\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    221\u001b[39m cos, sin = position_embeddings\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m query_states, key_states = \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    226\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:127\u001b[39m, in \u001b[36mapply_rotary_pos_emb\u001b[39m\u001b[34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[39m\n\u001b[32m    125\u001b[39m cos = cos.unsqueeze(unsqueeze_dim)\n\u001b[32m    126\u001b[39m sin = sin.unsqueeze(unsqueeze_dim)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m q_embed = (q * cos) + (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m * sin)\n\u001b[32m    128\u001b[39m k_embed = (k * cos) + (rotate_half(k) * sin)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mount/arbeitsdaten/studenten4/ashousaa/.env/lib64/python3.13/site-packages/transformers/models/qwen3/modeling_qwen3.py:98\u001b[39m, in \u001b[36mrotate_half\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     94\u001b[39m         down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrotate_half\u001b[39m(x):\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m     x1 = x[..., : x.shape[-\u001b[32m1\u001b[39m] // \u001b[32m2\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "k = 10\n",
    "correct_answers_count = 0\n",
    "\n",
    "encoding_used=\"mpnet\"\n",
    "model_shortname = model_name.split(\"/\")[-1]  # Remove slash inside the name cause gives error\n",
    "\n",
    "# ======= SELECT PROMPT TO USE =======\n",
    "\n",
    "\n",
    "# # --- Prompt 1: Basic RAG Prompt ---\n",
    "prompt_id = \"Basic_RAG_Prompt\"\n",
    "def build_prompt(context, query):\n",
    "    return f\"Context: {context} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "\n",
    "# # # --- Prompt 2: Chain-of-Thought (CoT) ---\n",
    "# prompt_id = \"COT_Prompt\"\n",
    "# def build_prompt(context, query):\n",
    "#     return (\n",
    "#         f\"Based on the retrieved context: {context}, \"\n",
    "#         f\"answer the question {query} step by step, first identifying key facts, \"\n",
    "#         f\"then reasoning through the answer.\"\n",
    "#     )\n",
    "\n",
    "# # # --- Prompt 3: No Context Provided ---\n",
    "# prompt_id = \"No_Context_Prompt\"\n",
    "# def build_prompt(context, query):\n",
    "#     return f\"Answer the query: {query} \\n Answer:\"\n",
    "\n",
    "# =====================================\n",
    "\n",
    "# === Load test set ===\n",
    "# with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     full_data = list(reader)\n",
    "with open(\"NEW_modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    full_data = list(reader)\n",
    "\n",
    "queries_chunk_map = [ \n",
    "    {\n",
    "        \"chunk_id\": row[\"Chunk\"],\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"reference\": row[\"Answer\"],\n",
    "        \"type\": row[\"Type\"],  \n",
    "        \"source_QID\": row[\"Source_QID\"]\n",
    "    }\n",
    "    \n",
    "    for row in full_data\n",
    "]\n",
    "\n",
    "# === Output files ===\n",
    "\n",
    "# # For trying different model sizes\n",
    "# csv_path = f\"NEW_RQ1_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# For trying different generation prompts\n",
    "# csv_path = f\"NEW_RQ2_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# For trying enabling thinking\n",
    "\n",
    "# # Testing enabling thinking with different model sizes\n",
    "# csv_path = f\"NEW_RQ3_size_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# # # Testing enabling thinking with different prompts\n",
    "# csv_path = f\"NEW_RQ3_prompt_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "with open(csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = [\n",
    "       \"Generation Model\", \"Question Index\", \"Question\", \"Type\", \"Source_QID\", \"Original Chunk\",\n",
    "        \"Chunks Retrieved\", \"Generated Answer\", \"Reference Answer\", \"Generation Prompt Used\", \"Encoding Used\",\"Enable Thinking\"\n",
    "    ]\n",
    "    \n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames) \n",
    "    # Creates a CSV writer object that will write dictionaries to the CSV file. \n",
    "    # The keys of the dictionaries are the fieldnames which are column headers in the CSV file.\n",
    "    # The values of the dictionaries are the data that will be written to the CSV file, specified when we call writer.writerow().\n",
    "\n",
    "    writer.writeheader() # Actually writes the header row to the CSV file.\n",
    "\n",
    "    for i, item in enumerate(queries_chunk_map, start=1):\n",
    "        chunk_id = item[\"chunk_id\"]\n",
    "        query = item[\"question\"]\n",
    "        reference_answer = item[\"reference\"]\n",
    "\n",
    "        # Get ids of top-k chunks\n",
    "        top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "\n",
    "        # Load corressponding chunk texts\n",
    "        # csv_file_path = '/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv'\n",
    "        csv_file_path = '/mount/arbeitsdaten/studenten4/ashousaa/NEW_chunks.csv'\n",
    "        result = get_CSVcolumn(csv_file_path, 'chunk_text')\n",
    "        # The following line, takes each chunk ID from the top_k_chunk_ids, extracts the numeric part of it and converts to it , and substracts one to be 0-based index. Then it retrieves the text from 'result'.\n",
    "        top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids] \n",
    "    \n",
    "\n",
    "        # Build prompt\n",
    "        top_k_string = \"\\n\".join(top_k_documents)\n",
    "        context = top_k_string if \"No_Context\" not in prompt_id else \"\"\n",
    "        print (f\"Context: {context}\")  # Debugging line to see the context being used\n",
    "        prompt_text = build_prompt(context, query)\n",
    "        # print(f\"Prompt text: {prompt_text}\")  # Debugging line to see the prompt text\n",
    "\n",
    "        # Generate response\n",
    "        raw_response = generate(\n",
    "            system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "            user_prompt=prompt_text,\n",
    "            temperature=0.7,\n",
    "            seed=42\n",
    "        ).strip()\n",
    "\n",
    "        # Correct chunk detection\n",
    "        normalized_chunk_id = normalize_string(chunk_id)\n",
    "        normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "        correct_chunk_found = normalized_chunk_id in normalized_top_k_chunk_ids\n",
    "        if correct_chunk_found:\n",
    "            correct_answers_count += 1\n",
    "\n",
    "\n",
    "        # Write to CSV\n",
    "        writer.writerow({\n",
    "            \"Generation Model\": model_name,\n",
    "            \"Question Index\": i,\n",
    "            \"Question\": query,\n",
    "            \"Type\": item[\"type\"],\n",
    "            \"Source_QID\": item[\"source_QID\"],\n",
    "            \"Original Chunk\": chunk_id,\n",
    "            \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "            \"Generated Answer\": raw_response,\n",
    "            \"Reference Answer\": reference_answer,\n",
    "            \"Generation Prompt Used\": prompt_id,\n",
    "            \"Encoding Used\": encoding_used,\n",
    "            \"Enable Thinking\": enable_thinking\n",
    "               \n",
    "        })\n",
    "\n",
    "# Print the number of correct answers\n",
    "# print(f\"✅ Finished {model_name}: {correct_answers_count})\")\n",
    "print(f\"✅ Finished {prompt_id}: {correct_answers_count})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
