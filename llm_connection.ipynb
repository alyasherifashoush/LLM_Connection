{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5476a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunk documents, save results to a dictionary where each chunk has an id and save the chunks to a CSV file\n",
    "\n",
    "import fitz  # Library for working with PDF files.\n",
    "import csv # Library for working with CSV files.\n",
    "\n",
    "def chunk_pdf_by_marker(pdf_path, marker=\"#\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract text from each page and concatenate\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        # print(full_text)\n",
    "\n",
    "    # Split by the marker\n",
    "    chunks = [chunk.strip() for chunk in full_text.split(marker) if chunk.strip()] # Splits the text by the marker and iterates on each chunk and cleans the whitespace. Also, it removes empty strings after cleaning.\n",
    "\n",
    "     # Store chunks in a dictionary\n",
    "    chunk_dict = {f\"chunk_{i+1}\": chunk for i, chunk in enumerate(chunks)} # After the colon is the value of the dictionary, which is the chunk of text. The key is the chunk number, which is created by iterating over the chunks and adding 1 to the index.\n",
    "\n",
    "    return chunk_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"Doc 4 flat.pdf\"\n",
    "chunk_dict = chunk_pdf_by_marker(pdf_path)\n",
    "\n",
    "# Print the first 3 chunks entirely\n",
    "print(\"--- First 3 Chunks ---\")\n",
    "for key, value in list(chunk_dict.items())[:4]:  # preview first 3 chunks\n",
    "    print(f\"{key}:\\n{value}\\n{'='*30}\\n\")\n",
    "\n",
    "# Also save chunks with ids as a csv file\n",
    "def save_chunks_to_csv(chunks, output_file=\"chunks.csv\"):\n",
    "    with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"chunk_id\", \"chunk_text\"])  # Write header\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i + 1}\"\n",
    "            writer.writerow([chunk_id, chunk])\n",
    "            writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "save_chunks_to_csv(list(chunk_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate questions using the LLM and save to a text file named \"response.txt\"\n",
    "with open(\"response.txt\", \"w\") as f:\n",
    "    for id in chunk_dict:\n",
    "        doc = chunk_dict[id]\n",
    "        # define prompts\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are about to go on a bussiness trip and want to ask very precise questions. Only output the question, no additional information.\"},\n",
    "            # {\"role\": \"user\", \"content\": f\"Imagine you are planning a business trip. What five questions would you ask after reading this document? {doc} Do not include the question if it is not stated in the chunk\" },  \n",
    "            # {\"role\": \"user\", \"content\": f\"Generate a list of natural-sounding questions that a traveling employee would have. The questions should reflect a genuine need to understand this information for their reimbursement claim.Only generate questions for which the corresponding answer is explicitly present here.{doc}\"} \n",
    "            {\"role\": \"user\", \"content\": f\"What specific questions would I, as a traveling employee, ask that could be answered *solely by the information provided in this chunk*? Only generate questions for which the corresponding answer is explicitly present here.{doc}\"}\n",
    "            # {\"role\": \"user\", \"content\": f\"As a traveling employee, what are some key, natural questions I would have to understand my rights, responsibilities, and potential reimbursements related to this specific information? Please generate questions that reflect a genuine need for clarity on how this impacts my expense report and reimbursement. Ensure the answer to each question is explicitly stated within this chunk, and format each question on a new line, numbered as in the examples you provided {doc}\"}  \n",
    "              ] \n",
    "        \n",
    "        # send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=messages,\n",
    "                    seed=42,\n",
    "                    temperature=0.7,\n",
    "            )\n",
    "      \n",
    "        # Get and clean the response\n",
    "        raw_response = response.choices[0].message.content.strip() \n",
    "\n",
    "        # Clean up the LLM's response by splitting into lines, removing leading/trailing noise, and filtering out empty lines\n",
    "        questions = [q.strip(\"0123456789).:- \") for q in raw_response.split(\"\\n\") if q.strip()] \n",
    "\n",
    "        # Write each question on a new line with the chunk ID\n",
    "        for question in questions:\n",
    "            f.write(f\"{question} || {id}\\n\") \n",
    "\n",
    "        # Add a blank line to separate questions from different chunks\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c6bce",
   "metadata": {},
   "source": [
    "Code to iterate over models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f327c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "# def load_model(model_name):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         torch_dtype=\"auto\",\n",
    "#         cache_dir=MODEL_CACHE_DIR\n",
    "#     ).to(\"cuda:0\")\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(system_prompt: str, user_prompt: str, model, tokenizer, temperature: float = 0.7, seed: int = 42, enable_thinking=False) -> str:\n",
    "#     set_seed(seed=seed)\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt}\n",
    "#     ]\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "#     generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "#     output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "#     content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "#     return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gc\n",
    "# import torch\n",
    "\n",
    "\n",
    "# k = 10\n",
    "# prompt_id = \"v1\"\n",
    "\n",
    "# # Models to iterate over\n",
    "# model_names = [\n",
    "#     \"Qwen/Qwen3-0.6B\",\n",
    "#     \"Qwen/Qwen3-1.7B\",\n",
    "#     \"Qwen/Qwen3-4B\",\n",
    "#     \"Qwen/Qwen3-8B\",\n",
    "# ]\n",
    "\n",
    "# # Load test set\n",
    "# with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     full_data = list(reader)\n",
    "\n",
    "# queries_chunk_map = [\n",
    "#     {\n",
    "#         \"chunk_id\": row[\"Chunk\"],\n",
    "#         \"question\": row[\"Question\"],\n",
    "#         \"reference\": row[\"Answer\"]\n",
    "#     }\n",
    "#     for row in full_data\n",
    "# ]\n",
    "\n",
    "# # Output files\n",
    "# output_txt_path = \"RAG_Output_Answers_ALL_MODELS.txt\"\n",
    "# output_csv_path = \"RAG_Output_Answers_ALL_MODELS.csv\"\n",
    "\n",
    "# with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file, \\\n",
    "#      open(output_csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "\n",
    "#     fieldnames = [\n",
    "#         \"Model Used\", \"Question Index\", \"Question\", \"Original Chunk\",\n",
    "#         \"Chunks Retrieved\", \"Answer\", \"Reference Answer\", \"Prompt Used\"\n",
    "#     ]\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for model_name in model_names:\n",
    "#         print(f\"\\nðŸ”„ Loading model: {model_name}\")\n",
    "#         model, tokenizer = load_model(model_name)\n",
    "\n",
    "#         correct_answers_count = 0\n",
    "\n",
    "#         for i, item in enumerate(queries_chunk_map, start=1):\n",
    "#             chunk_id = item[\"chunk_id\"]\n",
    "#             query = item[\"question\"]\n",
    "#             reference_answer = item[\"reference\"]\n",
    "\n",
    "#             top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "#             result = get_CSVcolumn('/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv', 'chunk_text')\n",
    "#             top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids]\n",
    "#             top_k_string = \"\\n\".join(top_k_documents)\n",
    "\n",
    "#             prompt_text = f\"Context: {top_k_string} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "#             raw_response = generate(\n",
    "#                 system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "#                 user_prompt=prompt_text,\n",
    "#                 model=model,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 temperature=0.7,\n",
    "#                 seed=42\n",
    "#             ).strip()\n",
    "\n",
    "#             # Evaluate if correct chunk was found\n",
    "#             normalized_chunk_id = normalize_string(chunk_id)\n",
    "#             normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "#             if normalized_chunk_id in normalized_top_k_chunk_ids:\n",
    "#                 correct_answers_count += 1\n",
    "\n",
    "#             # Write to text file\n",
    "#             txt_file.write(f\"[{model_name}] Question {i} (Original Chunk ID: {chunk_id})\\n\")\n",
    "#             txt_file.write(f\"Question: {query}\\n\")\n",
    "#             txt_file.write(f\"Top-k Chunks Used: {top_k_chunk_ids}\\n\")\n",
    "#             txt_file.write(f\"Answer: {raw_response}\\n\\n\")\n",
    "\n",
    "#             # Write to CSV\n",
    "#             writer.writerow({\n",
    "#                 \"Model Used\": model_name,\n",
    "#                 \"Question Index\": i,\n",
    "#                 \"Question\": query,\n",
    "#                 \"Original Chunk\": chunk_id,\n",
    "#                 \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "#                 \"Answer\": raw_response,\n",
    "#                 \"Reference Answer\": reference_answer,\n",
    "#                 \"Prompt Used\": prompt_id\n",
    "#             })\n",
    "\n",
    "#         print(f\"âœ… Finished {model_name}: {correct_answers_count}/{len(queries_chunk_map)} correct chunks\")\n",
    "\n",
    "#         # ðŸ§¹ Free memory\n",
    "#         del model\n",
    "#         del tokenizer\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaee8a",
   "metadata": {},
   "source": [
    "Code to Generate Model By Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f99b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_methods import get_CSVcolumn, normalize_string, get_top_matching_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0628f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Imports the os module, which provides a way to interact with the operating system.\n",
    "\n",
    "MODEL_CACHE_DIR = '/mount/arbeitsdaten/asr-2/vaethdk/resources/weights/llm/students' # Tells the Hugging Face Transformers library to store downloaded model components in that specific directory.\n",
    "os.environ['TRANSFORMERS_CACHE'] = MODEL_CACHE_DIR # Sets an environment variable that tells the transformers library to use the specified cache directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d7e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Imports necessary classes from the transformers library which is a library that provides pre-trained models and tokenizers for natural language processing tasks.\n",
    "# The following classes are imported:\n",
    "# 1. AutoModelForCausalLM: A generic class that can automatically load pre-trained language models for text generation (causal language modeling).\n",
    "# 2. AutoTokenizer: A generic class that can automatically load the appropriate tokenizer for a given pre-trained model. Tokenizers are used to convert text into numerical tokens that the model can understand and vice versa.\n",
    "# 3. set_seed: A function to set the random seed for reproducibility of the generation process.\n",
    "# 4. pipeline: A high-level API for using pre-trained models for various tasks, such as text generation, translation, etc.\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed ,pipeline\n",
    "\n",
    "\n",
    "# The model name points to a repository on huggingface, where it will load the model and configuration from, e.g.: https://huggingface.co/Qwen/Qwen3-0.6B\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# model_name = \"Qwen/Qwen3-1.7B\"\n",
    "# model_name = \"Qwen/Qwen3-4B\"\n",
    "# model_name = \"Qwen/Qwen3-8B\"\n",
    "# model_name = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# model_name = \"microsoft/Phi-3-small-128k-instruct\"\n",
    "model_name = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "\n",
    "\n",
    "# Load the tokenizer and the model (you can leave this block unchanged)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE_DIR) #trust_remote_code=True) # Loads the tokenizer for the specified model from the cache directory.\n",
    "model = AutoModelForCausalLM.from_pretrained(  # This is the key function from the transformers library that handles the loading process.\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\", # This argument specifies the data type of the model parameters. \"auto\" will automatically choose the best data type based on the hardware.\n",
    "    cache_dir=MODEL_CACHE_DIR, #This optional argument specifies a local directory where the downloaded model files (configuration, weights, etc.) will be stored.\n",
    "    # trust_remote_code=True, # This argument allows the model to use remote code for custom layers or configurations, which is necessary for some models that require specific implementations.\n",
    "    \n",
    ").to(\"cuda:0\") # This is a PyTorch method that moves the entire loaded model (all its parameters and buffers) onto the first CUDA-enabled GPU available on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67da31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is for:\n",
    "# 1. converting the queries into the right template format\n",
    "# 2. tokenizing the prompt text\n",
    "# 3. converting the tokens to token ids\n",
    "# 4. running the token ids through the model, generating output token ids\n",
    "# 5. converting the output token ids back to text tokens\n",
    "\n",
    "\n",
    "enable_thinking=False\n",
    "\n",
    "def generate(system_prompt: str, user_prompt: str, temperature: float = 0.7, seed: int = 42, enable_thinking=enable_thinking) -> str:\n",
    "    # set a random seed for reproducability (otherwise, calling generation twice can result in different texts)\n",
    "    set_seed(seed=seed)\n",
    "\n",
    "    # Convert the prompts into the correct template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template( # Applies the specific chat template defined for the Qwen model to format the messages into a single text string that the model expects as input. \n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True, # Adds a special token that signals the start of the generation.\n",
    "        enable_thinking=enable_thinking # A parameter specific to some Qwen models to control a \"thinking\" step during generation.\n",
    "    )\n",
    "    # Convert prompt texts to tokens, move the token ids to the GPU\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Passes the input tokens and set max new tokens number to finally generate the output token ids\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # Convert the output token ids to text tokens\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdda9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished Basic_RAG_Prompt: 137)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "k = 10\n",
    "correct_answers_count = 0\n",
    "\n",
    "encoding_used=\"mpnet\"\n",
    "model_shortname = model_name.split(\"/\")[-1]  # Remove slash inside the name cause gives error\n",
    "\n",
    "# ======= SELECT PROMPT TO USE =======\n",
    "\n",
    "\n",
    "# --- Prompt 1: Basic RAG Prompt ---\n",
    "prompt_id = \"Basic_RAG_Prompt\"\n",
    "def build_prompt(context, query):\n",
    "    return f\"Context: {context} \\n Question: {query} \\n Answer the question using only the context provided.\"\n",
    "\n",
    "\n",
    "# # --- Prompt 2: Chain-of-Thought (CoT) ---\n",
    "# prompt_id = \"COT_Prompt\"\n",
    "# def build_prompt(context, query):\n",
    "#     return (\n",
    "#         f\"Based on the retrieved context: {context}, \"\n",
    "#         f\"answer the question {query} step by step, first identifying key facts, \"\n",
    "#         f\"then reasoning through the answer.\"\n",
    "#     )\n",
    "\n",
    "# # --- Prompt 3: No Context Provided ---\n",
    "# prompt_id = \"No_Context_Prompt\"\n",
    "# def build_prompt(context, query):\n",
    "#     return f\"Answer the query: {query} \\n Answer:\"\n",
    "\n",
    "# =====================================\n",
    "\n",
    "# === Load test set ===\n",
    "with open(\"modified_final_test_set.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    full_data = list(reader)\n",
    "\n",
    "\n",
    "queries_chunk_map = [ \n",
    "    {\n",
    "        \"chunk_id\": row[\"Chunk\"],\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"reference\": row[\"Answer\"],\n",
    "        \"type\": row[\"Type\"],  \n",
    "        \"source_QID\": row[\"Source_QID\"]\n",
    "    }\n",
    "    \n",
    "    for row in full_data\n",
    "]\n",
    "\n",
    "# === Output files ===\n",
    "\n",
    "# For trying different model sizes\n",
    "csv_path = f\"RQ1_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# # For trying different generation prompts\n",
    "# csv_path = f\"RQ2_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# For trying enabling thinking\n",
    "\n",
    "# Testing enabling thinking with different model sizes\n",
    "# csv_path = f\"RQ3_size_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "# # # Testing enabling thinking with different prompts\n",
    "# csv_path = f\"RQ3_prompt_RAG_Answers_{prompt_id}_{model_shortname}.csv\"\n",
    "\n",
    "with open(csv_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = [\n",
    "       \"Generation Model\", \"Question Index\", \"Question\", \"Type\", \"Source_QID\", \"Original Chunk\",\n",
    "        \"Chunks Retrieved\", \"Generated Answer\", \"Reference Answer\", \"Generation Prompt Used\", \"Encoding Used\",\"Enable Thinking\"\n",
    "    ]\n",
    "    \n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames) \n",
    "    # Creates a CSV writer object that will write dictionaries to the CSV file. \n",
    "    # The keys of the dictionaries are the fieldnames which are column headers in the CSV file.\n",
    "    # The values of the dictionaries are the data that will be written to the CSV file, specified when we call writer.writerow().\n",
    "\n",
    "    writer.writeheader() # Actually writes the header row to the CSV file.\n",
    "\n",
    "    for i, item in enumerate(queries_chunk_map, start=1):\n",
    "        chunk_id = item[\"chunk_id\"]\n",
    "        query = item[\"question\"]\n",
    "        reference_answer = item[\"reference\"]\n",
    "\n",
    "        # Get ids of top-k chunks\n",
    "        top_k_chunk_ids = get_top_matching_chunks(query, k)\n",
    "\n",
    "        # Load corressponding chunk texts\n",
    "        csv_file_path = '/mount/arbeitsdaten/studenten4/ashousaa/chunks.csv'\n",
    "        result = get_CSVcolumn(csv_file_path, 'chunk_text')\n",
    "        # The following line, takes each chunk ID from the top_k_chunk_ids, extracts the numeric part of it and converts to it , and substracts one to be 0-based index. Then it retrieves the text from 'result'.\n",
    "        top_k_documents = [result[int(c_id.split(\"_\")[1]) - 1] for c_id in top_k_chunk_ids] \n",
    "    \n",
    "\n",
    "        # Build prompt\n",
    "        top_k_string = \"\\n\".join(top_k_documents)\n",
    "        context = top_k_string if \"No_Context\" not in prompt_id else \"\"\n",
    "        prompt_text = build_prompt(context, query)\n",
    "\n",
    "        # Generate response\n",
    "        raw_response = generate(\n",
    "            system_prompt=\"Please answer the user question in a faithful way.\",\n",
    "            user_prompt=prompt_text,\n",
    "            temperature=0.7,\n",
    "            seed=42\n",
    "        ).strip()\n",
    "\n",
    "        # Correct chunk detection\n",
    "        normalized_chunk_id = normalize_string(chunk_id)\n",
    "        normalized_top_k_chunk_ids = [normalize_string(a) for a in top_k_chunk_ids]\n",
    "        correct_chunk_found = normalized_chunk_id in normalized_top_k_chunk_ids\n",
    "        if correct_chunk_found:\n",
    "            correct_answers_count += 1\n",
    "\n",
    "\n",
    "        # Write to CSV\n",
    "        writer.writerow({\n",
    "            \"Generation Model\": model_name,\n",
    "            \"Question Index\": i,\n",
    "            \"Question\": query,\n",
    "            \"Type\": item[\"type\"],\n",
    "            \"Source_QID\": item[\"source_QID\"],\n",
    "            \"Original Chunk\": chunk_id,\n",
    "            \"Chunks Retrieved\": \"; \".join(top_k_chunk_ids),\n",
    "            \"Generated Answer\": raw_response,\n",
    "            \"Reference Answer\": reference_answer,\n",
    "            \"Generation Prompt Used\": prompt_id,\n",
    "            \"Encoding Used\": encoding_used,\n",
    "            \"Enable Thinking\": enable_thinking\n",
    "               \n",
    "        })\n",
    "\n",
    "# Print the number of correct answers\n",
    "# print(f\"âœ… Finished {model_name}: {correct_answers_count})\")\n",
    "print(f\"âœ… Finished {prompt_id}: {correct_answers_count})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
