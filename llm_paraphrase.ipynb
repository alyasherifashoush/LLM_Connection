{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7dd7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters (only has to be executed once during a session)\n",
    "\n",
    "# Note: `localhost` is only valid as long as you are logged in on madagaskarweihe. \n",
    "# Otherwise, you will need to perform port forwarding on your machine using `ssh -fNL 8006:127.0.0.1:8006 ashousaa@madagaskarweihe`.\n",
    "# This might require an ssh key to be uploaded to madagaskarweihe first.\n",
    "\n",
    "# On the server where VLLM, a software framework designed to efficiently run Large Language Models (LLMs), is installed, the VLLM endpoint is the specific network address (like a website URL) that your program uses to communicate with the VLLM server, and the VLLM key is a secret code used to authenticate your program and grant it permission to access the LLMs managed by VLLM.\n",
    "# The chosen model is the default one.\n",
    "\n",
    "VLLM_API_ENDPOINT = 'http://localhost:8006/v1' \n",
    "VLLM_KEY = 's7Vtzeyq3kfhVkPhlWdL95pPRBq36KDP1d5bBj54BqQ'\n",
    "MODEL = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to LLM server (only has to be executed once during a session)\n",
    "# It creates a communication channel (client) to a specific language model server located at VLLM_API_ENDPOINT. To be allowed to use this server, your program provides the VLLM_KEY as proof of authorization. Once this client object is created, you can use its methods to send text prompts to the language model and receive its generated responses\n",
    "client = OpenAI(api_key=VLLM_KEY,\n",
    "                base_url=VLLM_API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split questions by ?\n",
    "import docx # Imports the python-docx library, which allows Python to interact with Microsoft Word .docx files.\n",
    "import csv\n",
    "\n",
    "def split_questions_by_marker(doc_path, marker=\"?\"):\n",
    "    doc = docx.Document(doc_path) #Opens the .docx file specified by doc_path using the docx.Document() constructor\n",
    "    full_text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text += paragraph.text + \"\\n\" # Appends the text of the current paragraph to the full_text string, followed by a newline character to ensure paragraph breaks are maintained.\n",
    "\n",
    "    # Split on \"?\" \n",
    "    raw_parts = full_text.split(marker) # Splits the full_text into a list of strings (raw_parts) using the specified marker (default \"?\") as the delimiter. The marker itself is not included in the resulting parts.\n",
    "    questions = []\n",
    "\n",
    "    for part in raw_parts:\n",
    "        cleaned = part.strip() # Remove leading/trailing whitespace\n",
    "        if cleaned:\n",
    "            questions.append(cleaned + marker)  # Add ? back to have question mark at the end of each question\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "doc_path = \"Manual Ques (after modification) .docx\"\n",
    "questions = split_questions_by_marker(doc_path)\n",
    "\n",
    "# # Save questions with ids as a csv file\n",
    "# def save_chunks_to_csv(questions, output_file=\"original_questions.csv\"):\n",
    "#     with open(output_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"question_id\", \"question_text\"])  # Write header\n",
    "\n",
    "#         for i, question in enumerate(questions):\n",
    "#             question_id = f\"question_{i + 1}\"\n",
    "#             writer.writerow([question_id, question])\n",
    "#             writer.writerow([]) # Empty row to create a line break\n",
    "\n",
    "# # Call the save_chunks_to_csv function with the values from the chunk_dict\n",
    "# save_chunks_to_csv(questions=questions, output_file=\"original_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prompt LLM To Paraphrase Questions\n",
    "# with open(\"paraphrased_questions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i, question in enumerate(questions):\n",
    "\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant that paraphrases questions. Do NOT repeat the question exactly. Do NOT add new information.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Paraphrase the following question: {question}\"}\n",
    "#         ]\n",
    "        \n",
    "#         # Send prompts and wait for answer\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=MODEL,\n",
    "#             messages=messages,\n",
    "#             seed=42,#  An initial value that initializes the random number generator in a generative model. For reproducibility you can set it to a fixed value.\n",
    "#             temperature=0.7, # A parameter that controls the randomness and creativity of a generative model's output.\n",
    "#         )\n",
    "\n",
    "#         # Extract and clean the LLM's response\n",
    "#         paraphrased = response.choices[0].message.content.strip() \n",
    "\n",
    "#         # Save paraphrased question\n",
    "#         f.write(f\"Q{i+1} Paraphrased: {paraphrased}\\n\\n\")\n",
    "\n",
    "\n",
    "# Prompt LLM To Paraphrase Questions\n",
    "with open(\"modified_paraphrased_questions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, question in enumerate(questions):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that paraphrases questions. Do NOT repeat the question exactly. Do NOT add new information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Paraphrase the following question: {question}\"}\n",
    "        ]\n",
    "        \n",
    "        # Send prompts and wait for answer\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            seed=42, # An initial value that initializes the random number generator in a generative model that controls which next word to choose . For reproducibility you can set it to a fixed value.\n",
    "            temperature=0.7, # A parameter that controls the randomness and creativity of a generative model's output.\n",
    "        )\n",
    "\n",
    "        # Extract and clean the LLM's response\n",
    "        # LLM response object -> selecting the first choice, here likely there are no others -> getting the message content -> remove leading/trailing whitespace\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove introductory lines like \"Here are a few ways...\"\n",
    "        lines = content.splitlines() # Split the content into lines\n",
    "        for line in lines:\n",
    "            line = line.strip() # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if line.startswith((\"*\", \"-\", \"â€¢\")): # Check if the line starts with a bullet point, dash, or asterisk incdicating it is paraphrased\n",
    "                line = line[1:].strip()\n",
    "                f.write(f\"Q{i+1} Paraphrased: {line}\\n\") \n",
    "\n",
    "            elif not line.lower().startswith(\"here are\"): # Check if the line starts with \"Here are\" (case insensitive)\n",
    "                f.write(f\"Q{i+1} Paraphrased: {line}\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
